{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 \u2014 Threshold Tuning & Expected Loss (Decision Policy)\n",
        "\n",
        "Convert risk scores (probabilities) into a **tiered decision policy**.\n",
        "\n",
        "**Key idea:** choose thresholds that minimize **expected loss**, not default 0.5.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "scoring = pd.read_csv('../data/xgb_test_scoring.csv')\n",
        "y_true = scoring['y_true'].astype(int).values\n",
        "proba = scoring['proba'].astype(float).values\n",
        "amount = scoring['amount'].astype(float).values\n",
        "\n",
        "print('Test size:', len(y_true))\n",
        "print('Fraud rate:', y_true.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Cost model\n",
        "\n",
        "Simple cost model for a portfolio-ready narrative:\n",
        "- **FN (missed fraud)** cost = missed transaction amount\n",
        "- **FP (wrongly intervened)** cost = fixed friction + ops cost (`FP_COST`)\n",
        "\n",
        "Tune `FP_COST` to reflect how strict you want the system to be.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "FP_COST = 10.0\n",
        "\n",
        "def expected_loss_at_threshold(t, y, p, amt, fp_cost=FP_COST):\n",
        "    pred = (p >= t).astype(int)\n",
        "    fp = ((pred==1) & (y==0)).sum()\n",
        "    fn_mask = ((pred==0) & (y==1))\n",
        "    fn = fn_mask.sum()\n",
        "    fn_cost = amt[fn_mask].sum()\n",
        "    loss = fp * fp_cost + fn_cost\n",
        "    return loss, fp, fn, fn_cost\n",
        "\n",
        "thresholds = np.linspace(0, 1, 501)\n",
        "rows = []\n",
        "for t in thresholds:\n",
        "    loss, fp, fn, fn_cost = expected_loss_at_threshold(t, y_true, proba, amount)\n",
        "    rows.append((t, loss, fp, fn, fn_cost))\n",
        "\n",
        "loss_df = pd.DataFrame(rows, columns=['threshold','expected_loss','fp','fn','fn_cost_amount'])\n",
        "best = loss_df.loc[loss_df['expected_loss'].idxmin()]\n",
        "best"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(loss_df['threshold'], loss_df['expected_loss'])\n",
        "plt.title('Expected Loss vs Threshold')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Expected Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Tiered policy: Allow / Flag / Block\n",
        "\n",
        "- `t_flag`: start flagging for review\n",
        "- `t_block`: block only the top-risk tail\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "t_flag = float(best['threshold'])\n",
        "block_top_pct = 0.001  # top 0.1% risk\n",
        "t_block = float(np.quantile(proba, 1 - block_top_pct))\n",
        "print('t_flag:', t_flag)\n",
        "print('t_block:', t_block)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def assign_action(p, t_flag, t_block):\n",
        "    return np.where(p >= t_block, 2, np.where(p >= t_flag, 1, 0))\n",
        "\n",
        "actions = assign_action(proba, t_flag, t_block)\n",
        "action_map = {0:'Allow', 1:'Flag', 2:'Block'}\n",
        "print(pd.Series(actions).map(action_map).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Evaluate tiered policy (Flag+Block => intervention)\n",
        "intervene = (actions >= 1).astype(int)\n",
        "tp = ((intervene==1) & (y_true==1)).sum()\n",
        "fp = ((intervene==1) & (y_true==0)).sum()\n",
        "fn = ((intervene==0) & (y_true==1)).sum()\n",
        "tn = ((intervene==0) & (y_true==0)).sum()\n",
        "\n",
        "precision = tp / max(tp + fp, 1)\n",
        "recall = tp / max(tp + fn, 1)\n",
        "fn_cost = amount[(intervene==0) & (y_true==1)].sum()\n",
        "policy_loss = fp * FP_COST + fn_cost\n",
        "\n",
        "print('TP:', tp, 'FP:', fp, 'FN:', fn, 'TN:', tn)\n",
        "print(f'Precision: {precision:.4f} | Recall: {recall:.4f}')\n",
        "print(f'Expected Loss: {policy_loss:.2f}')\n",
        "\n",
        "block_mask = (actions == 2)\n",
        "print('Block count:', int(block_mask.sum()))\n",
        "if block_mask.sum() > 0:\n",
        "    print('Block fraud rate:', float(y_true[block_mask].mean()))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Export decision policy output\n",
        "out = scoring.copy()\n",
        "out['action'] = pd.Series(actions).map(action_map)\n",
        "out.to_csv('../data/decision_policy_output.csv', index=False)\n",
        "print('Saved ../data/decision_policy_output.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next: Notebook 05 uses SHAP to provide a global explanation for the model.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}